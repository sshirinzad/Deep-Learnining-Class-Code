{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr1FPso5Thh12RkpAy3RsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sshirinzad/Deep-Learnining-Class-Code/blob/main/Python_Snippet_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function for backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize the weights for input-to-hidden, hidden-to-hidden, and hidden-to-output layers\n",
        "def init_weights(n_inputs, n_hidden, n_output):\n",
        "    W0 = np.random.randn(n_inputs, n_hidden)  # Input to hidden layer\n",
        "    W1 = np.random.randn(n_hidden, n_hidden)  # Hidden to hidden layer\n",
        "    W2 = np.random.randn(n_hidden, n_output)  # Hidden to output layer\n",
        "    return W0, W1, W2\n",
        "\n",
        "# Feedforward operation\n",
        "def feedforward(x, W0, W1, W2):\n",
        "    z0 = np.dot(x, W0)  # Input to first hidden layer\n",
        "    a0 = sigmoid(z0)    # Activation of the first hidden layer\n",
        "\n",
        "    z1 = np.dot(a0, W1) # First hidden to second hidden layer\n",
        "    a1 = sigmoid(z1)    # Activation of the second hidden layer\n",
        "\n",
        "    z2 = np.dot(a1, W2) # Second hidden to output layer\n",
        "    a2 = sigmoid(z2)    # Activation of the output layer\n",
        "\n",
        "    return z0, a0, z1, a1, z2, a2\n",
        "\n",
        "# Predict function (just run feedforward and return the output)\n",
        "def predict(x, W0, W1, W2):\n",
        "    _, _, _, _, _, a2 = feedforward(x, W0, W1, W2)\n",
        "    return a2\n",
        "\n",
        "# Backpropagation to update the weights\n",
        "def backpropagate(x, y, W0, W1, W2, z0, a0, z1, a1, z2, a2, learning_rate):\n",
        "    # Output layer error\n",
        "    output_error = y - a2\n",
        "    output_delta = output_error * sigmoid_derivative(a2)\n",
        "\n",
        "    # Hidden layer error (second hidden layer)\n",
        "    hidden1_error = np.dot(output_delta, W2.T)\n",
        "    hidden1_delta = hidden1_error * sigmoid_derivative(a1)\n",
        "\n",
        "    # Hidden layer error (first hidden layer)\n",
        "    hidden0_error = np.dot(hidden1_delta, W1.T)\n",
        "    hidden0_delta = hidden0_error * sigmoid_derivative(a0)\n",
        "\n",
        "    # Update weights\n",
        "    W2 += np.dot(a1.T, output_delta) * learning_rate\n",
        "    W1 += np.dot(a0.T, hidden1_delta) * learning_rate\n",
        "    W0 += np.dot(x.T, hidden0_delta) * learning_rate\n",
        "\n",
        "    return W0, W1, W2\n",
        "\n",
        "# Train the neural network using gradient descent and backpropagation\n",
        "def train(X_train, Y_train, n_inputs, n_hidden, n_output, n_epochs, learning_rate):\n",
        "    # Initialize weights\n",
        "    W0, W1, W2 = init_weights(n_inputs, n_hidden, n_output)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for i in range(len(X_train)):\n",
        "            x = np.reshape(X_train[i], (1, n_inputs))\n",
        "            y = np.reshape(Y_train[i], (1, n_output))\n",
        "\n",
        "            # Feedforward\n",
        "            z0, a0, z1, a1, z2, a2 = feedforward(x, W0, W1, W2)\n",
        "\n",
        "            # Backpropagation\n",
        "            W0, W1, W2 = backpropagate(x, y, W0, W1, W2, z0, a0, z1, a1, z2, a2, learning_rate)\n",
        "\n",
        "        # Print the loss at every epoch\n",
        "        if epoch % 100 == 0:\n",
        "            loss = np.mean(np.square(Y_train - predict(X_train, W0, W1, W2)))\n",
        "            print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    return W0, W1, W2\n",
        "\n",
        "# Test the implementation with the provided example\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate toy data\n",
        "    X_train = np.random.randn(1000, 10)  # 1000 examples, 10 input features\n",
        "    Y_train = np.random.randn(1000, 3)   # 1000 examples, 3 output features\n",
        "\n",
        "    # Initialize network\n",
        "    n_inputs = 10\n",
        "    n_hidden = 5\n",
        "    n_output = 3\n",
        "\n",
        "    # Train the network\n",
        "    W0, W1, W2 = train(X_train, Y_train, n_inputs, n_hidden, n_output, n_epochs=1000, learning_rate=0.1)\n",
        "\n",
        "    # Test the network\n",
        "    x_test = np.random.randn(1, 10)  # Single test example\n",
        "    y_pred = predict(x_test, W0, W1, W2)\n",
        "    print(\"Prediction for the test example:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOCbRbcrpC3l",
        "outputId": "b9997f34-f6c6-4799-81cf-06a10257a313"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.3084298699987513\n",
            "Epoch 100, Loss: 1.0063569954587943\n",
            "Epoch 200, Loss: 0.9829849229680062\n",
            "Epoch 300, Loss: 0.9779430226425179\n",
            "Epoch 400, Loss: 0.9751699742327461\n",
            "Epoch 500, Loss: 0.9841768765240999\n",
            "Epoch 600, Loss: 0.9712727715015944\n",
            "Epoch 700, Loss: 0.9660829860968042\n",
            "Epoch 800, Loss: 0.9704730638995005\n",
            "Epoch 900, Loss: 0.9623553101906704\n",
            "Prediction for the test example: [[0.08116146 0.99996976 0.07122092]]\n"
          ]
        }
      ]
    }
  ]
}