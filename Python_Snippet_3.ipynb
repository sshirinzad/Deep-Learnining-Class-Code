{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaYqEJ0FmGnXphlO+Qx+NN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sshirinzad/Deep-Learnining-Class-Code/blob/main/Python_Snippet_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 12: Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "xTiuKZsaqP_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kzdQsW7app7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def mini_batch_gradient_descent(X, y, batch_size, learning_rate, num_iterations):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.random.randn(n_features)  # Randomly initializing the weight vector\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Shuffle data\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        # Mini-batch gradient descent\n",
        "        for j in range(0, n_samples, batch_size):\n",
        "            X_batch = X_shuffled[j:j + batch_size]\n",
        "            y_batch = y_shuffled[j:j + batch_size]\n",
        "\n",
        "            # Prediction and gradient computation\n",
        "            y_pred = np.dot(X_batch, weights)\n",
        "            gradient = -(2 / batch_size) * np.dot(X_batch.T, (y_batch - y_pred))\n",
        "\n",
        "            # Update weights\n",
        "            weights -= learning_rate * gradient\n",
        "\n",
        "        # Calculate cost (Mean Squared Error)\n",
        "        cost = np.mean((y - np.dot(X, weights)) ** 2)\n",
        "        costs.append(cost)\n",
        "\n",
        "        # Print cost every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Cost: {cost}\")\n",
        "\n",
        "    return costs, weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 13: Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "Yhq9d6F1qO4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.random.randn(n_features)  # Randomly initializing the weight vector\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Shuffle the data\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        # Stochastic gradient descent (one data point at a time)\n",
        "        for j in range(n_samples):\n",
        "            x_j = X_shuffled[j].reshape(1, -1)\n",
        "            y_j = y_shuffled[j]\n",
        "\n",
        "            # Prediction and gradient computation\n",
        "            y_pred = np.dot(x_j, weights)\n",
        "            gradient = -(2) * np.dot(x_j.T, (y_j - y_pred))\n",
        "\n",
        "            # Update the weights\n",
        "            weights -= learning_rate * gradient\n",
        "\n",
        "        # Print progress every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            cost = np.mean((y - np.dot(X, weights)) ** 2)\n",
        "            print(f\"Iteration {i}, Cost: {cost}\")\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "4nn_ING_rKGk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 14: Gradient Descent with Momentum"
      ],
      "metadata": {
        "id": "TjClcfK-rSOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent_with_momentum(w_init, data, learning_rate, beta, num_iters):\n",
        "    weights = np.array(w_init)\n",
        "    v = np.zeros_like(weights)  # Initialize velocity (momentum)\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        grad_sum = np.zeros_like(weights)\n",
        "\n",
        "        # Loop over each training example\n",
        "        for x, y in data:\n",
        "            x = np.array(x)\n",
        "            y_pred = np.dot(x, weights)\n",
        "            error = y_pred - y\n",
        "            gradient = x * error\n",
        "            grad_sum += gradient\n",
        "\n",
        "        # Momentum update\n",
        "        v = beta * v + (1 - beta) * grad_sum\n",
        "        weights -= learning_rate * v\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            cost = np.mean([0.5 * (np.dot(np.array(x), weights) - y) ** 2 for x, y in data])\n",
        "            print(f\"Iteration {i}, Cost: {cost}\")\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "rsamyLaSrXJ2"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}